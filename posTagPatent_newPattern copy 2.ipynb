{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "string.punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \n",
       "0  The present disclosure relates to a fire detec...  \n",
       "1  The present disclosure relates to a fire detec...  \n",
       "2  Responsive to a CPU load of a specific access ...  \n",
       "3  Responsive to a CPU load of a specific access ...  \n",
       "4  Methods, apparatuses and computer readable med...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data (data yang digunakan adalah data sample 500 baris agar pemrosesannya tidak lama)\n",
    "\n",
    "data = pd.read_csv('data/patent.csv')\n",
    "data = data[[\"title\", \"abstract_localized.text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "      <th>title_and_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \\\n",
       "0  The present disclosure relates to a fire detec...   \n",
       "1  The present disclosure relates to a fire detec...   \n",
       "2  Responsive to a CPU load of a specific access ...   \n",
       "3  Responsive to a CPU load of a specific access ...   \n",
       "4  Methods, apparatuses and computer readable med...   \n",
       "\n",
       "                                  title_and_abstract  \n",
       "0  Fire detection system based on artificial inte...  \n",
       "1  The present disclosure relates to a fire detec...  \n",
       "2  Embedding artificial intelligence for balancin...  \n",
       "3  Responsive to a CPU load of a specific access ...  \n",
       "4  Optimization techniques for artificial intelli...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add . in the end of title\n",
    "def add_period(text):\n",
    "    if pd.notna(text):\n",
    "        return text + \". \"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the 'title' column\n",
    "data['title'] = data['title'].apply(add_period)\n",
    "\n",
    "# merge title and abstract column\n",
    "data['title_and_abstract'] = data['title'].fillna('') + data['abstract_localized.text']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                      234\n",
      "abstract_localized.text      0\n",
      "title_and_abstract           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing value\n",
    "\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove missing value (tidak perlu karena kolom title and abstract tidak memiliki missing value)\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fire detection system based on artificial inte...\n",
       "1    the present disclosure relates to a fire detec...\n",
       "2    embedding artificial intelligence for balancin...\n",
       "3    responsive to a cpu load of a specific access ...\n",
       "4    optimization techniques for artificial intelli...\n",
       "Name: title_and_abstract, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowering text\n",
    "data['title_and_abstract']= data['title_and_abstract'].apply(lambda x: x.lower())\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['title_and_abstract'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fire', 'detection', 'system', 'based', 'on', 'artificial', 'intelligence', 'and', 'fire', 'detection', 'method', 'based', 'on', 'artificial', 'intelligence'], ['the', 'present', 'disclosure', 'relates', 'to', 'a', 'fire', 'detection', 'system', 'based', 'on', 'ai', 'and', 'a', 'fire', 'detection', 'method', 'based', 'on', 'ai', 'which', 'detects', 'fire', 'by', 'using', 'deep', 'learning'], ['the', 'fire', 'detection', 'system', 'includes', 'a', 'camera', 'part', 'generating', 'video', 'images', 'an', 'object', 'extraction', 'part', 'extracting', 'a', 'motion', 'object', 'from', 'the', 'video', 'images', 'a', 'video', 'image', 'conversion', 'part', 'generating', 'a', 'first', 'background', 'removal', 'image', 'an', 'image', 'division', 'part', 'dividing', 'the', 'first', 'background', 'removal', 'image', 'with', 'a', 'plurality', 'of', 'division', 'lines', 'and', 'an', 'analyzing', 'part', 'generating', 'an', 'abnormal', 'signal', 'and', 'a', 'normal', 'signal']]\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize each sentence, tokenize each word from sentence, then remove stopwords\n",
    "def tokenize_and_remove_punctuation(text):\n",
    "    # Tokenize each sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and tokenize each word in each sentence\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each sentence and remove punctuation\n",
    "data['tokenized'] = data['title_and_abstract'].apply(tokenize_and_remove_punctuation)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['tokenized'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')], [('the', 'DT'), ('present', 'JJ'), ('disclosure', 'NN'), ('relates', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('and', 'CC'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('which', 'WDT'), ('detects', 'VBZ'), ('fire', 'NN'), ('by', 'IN'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'NN')], [('the', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('includes', 'VBZ'), ('a', 'DT'), ('camera', 'NN'), ('part', 'NN'), ('generating', 'VBG'), ('video', 'NN'), ('images', 'NNS'), ('an', 'DT'), ('object', 'JJ'), ('extraction', 'NN'), ('part', 'NN'), ('extracting', 'VBG'), ('a', 'DT'), ('motion', 'NN'), ('object', 'NN'), ('from', 'IN'), ('the', 'DT'), ('video', 'NN'), ('images', 'VBZ'), ('a', 'DT'), ('video', 'NN'), ('image', 'NN'), ('conversion', 'NN'), ('part', 'NN'), ('generating', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('an', 'DT'), ('image', 'NN'), ('division', 'NN'), ('part', 'NN'), ('dividing', 'VBG'), ('the', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('with', 'IN'), ('a', 'DT'), ('plurality', 'NN'), ('of', 'IN'), ('division', 'NN'), ('lines', 'NNS'), ('and', 'CC'), ('an', 'DT'), ('analyzing', 'VBG'), ('part', 'NN'), ('generating', 'VBG'), ('an', 'DT'), ('abnormal', 'JJ'), ('signal', 'NN'), ('and', 'CC'), ('a', 'DT'), ('normal', 'JJ'), ('signal', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to perform POS tagging on each token\n",
    "def pos_tag_tokens(tokenized_sentences):\n",
    "    pos_tagged_sentences = []\n",
    "    for sentence_tokens in tokenized_sentences:\n",
    "        pos_tags = pos_tag(sentence_tokens)\n",
    "        pos_tagged_sentences.append(pos_tags)\n",
    "    return pos_tagged_sentences\n",
    "\n",
    "# Perform POS tagging on tokenized sentences\n",
    "data['pos_tagged'] = data['tokenized'].apply(pos_tag_tokens)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['pos_tagged'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')], [('the', 'DT'), ('present', 'JJ'), ('disclosure', 'NN'), ('relate', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('and', 'CC'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('which', 'WDT'), ('detect', 'VBZ'), ('fire', 'NN'), ('by', 'IN'), ('use', 'VBG'), ('deep', 'JJ'), ('learning', 'NN')], [('the', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('include', 'VBZ'), ('a', 'DT'), ('camera', 'NN'), ('part', 'NN'), ('generate', 'VBG'), ('video', 'NN'), ('image', 'NNS'), ('an', 'DT'), ('object', 'JJ'), ('extraction', 'NN'), ('part', 'NN'), ('extract', 'VBG'), ('a', 'DT'), ('motion', 'NN'), ('object', 'NN'), ('from', 'IN'), ('the', 'DT'), ('video', 'NN'), ('image', 'VBZ'), ('a', 'DT'), ('video', 'NN'), ('image', 'NN'), ('conversion', 'NN'), ('part', 'NN'), ('generate', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('an', 'DT'), ('image', 'NN'), ('division', 'NN'), ('part', 'NN'), ('divide', 'VBG'), ('the', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('with', 'IN'), ('a', 'DT'), ('plurality', 'NN'), ('of', 'IN'), ('division', 'NN'), ('line', 'NNS'), ('and', 'CC'), ('an', 'DT'), ('analyze', 'VBG'), ('part', 'NN'), ('generate', 'VBG'), ('an', 'DT'), ('abnormal', 'JJ'), ('signal', 'NN'), ('and', 'CC'), ('a', 'DT'), ('normal', 'JJ'), ('signal', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(pos_tagged_sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence_tags in pos_tagged_sentences:\n",
    "        lemmatized_tokens = [(lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos_tag)), pos_tag) for token, pos_tag in sentence_tags]\n",
    "        lemmatized_sentences.append(lemmatized_tokens)\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Function to map POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN  # Default to noun if POS tag not recognized\n",
    "\n",
    "# Lemmatize the tokens in 'pos_tagged_sentences' column\n",
    "data['lemmatized'] = data['pos_tagged'].apply(lemmatize_tokens)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['lemmatized'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(detect, fire), (generate, video), (analyze, ...\n",
       "1      [(detect, fire), (generate, video), (analyze, ...\n",
       "2      [(balance, access), (process, load), (enable, ...\n",
       "3                [(threshold, value), (determine, rssi)]\n",
       "4      [(receive, annotation), (determine, document),...\n",
       "                             ...                        \n",
       "495                [(learn, model), (further, comprise)]\n",
       "496    [(optimize, hyperparameter), (optimize, hyperp...\n",
       "497    [(optimize, hyperparameter), (ai, model), (eva...\n",
       "498    [(provide, system), (provide, ai), (compute, e...\n",
       "499    [(provide, system), (provide, ai), (compute, e...\n",
       "Name: chunked_verb_noun, Length: 500, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos chunk verb noun pairs\n",
    "\n",
    "def extract_verb_noun_pairs(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_sentence in tagged_texts:\n",
    "        result = filter_verb_noun_pairs(tagged_sentence)\n",
    "        chunks.append(result)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def filter_verb_noun_pairs(tagged_sentence):\n",
    "    verb_noun_pairs = []\n",
    "    for i in range(len(tagged_sentence) - 1):\n",
    "        word, tag = tagged_sentence[i]\n",
    "        next_word, next_tag = tagged_sentence[i + 1]\n",
    "        if tag.startswith('VB') and next_tag.startswith('NN'):\n",
    "            verb = word\n",
    "            noun = next_word\n",
    "            verb_noun_pairs.append((verb, noun))\n",
    "    return verb_noun_pairs\n",
    "\n",
    "# Apply chunking and extraction to the 'tagged_sentences' column\n",
    "data['chunked_verb_noun'] = data['lemmatized'].apply(extract_verb_noun_pairs)\n",
    "\n",
    "# Print the resulting DataFrame with chunked data\n",
    "data['chunked_verb_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(base, on, ai), (base, on, ai), (detect, fire...\n",
       "1      [(base, on, ai), (base, on, ai), (detect, fire...\n",
       "2      [(balance, access, point, process, load), (ena...\n",
       "3      [(threshold, value), (determine, rssi, value),...\n",
       "4      [(comprise, select, from, a, pool), (be, annot...\n",
       "                             ...                        \n",
       "495    [(comprise, receive, from, a, client, device),...\n",
       "496    [(optimize, hyperparameter, tuples), (optimize...\n",
       "497    [(optimize, hyperparameter, tuples), (ai, mode...\n",
       "498    [(be, provide, system), (provide, ai, system),...\n",
       "499    [(be, provide, system), (provide, ai, system),...\n",
       "Name: chunked_new_pattern, Length: 500, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS chunk new pattern (Pak Indra)\n",
    "\n",
    "# define pattern\n",
    "pattern = r'''Chunk: {(<ADJ|VB\\w*>+<\\.|RP|IN|CC|PRP\\w*>*<DT>*<NN\\w*|RB\\w*>+)+}'''\n",
    "chunk_parser = RegexpParser(pattern)\n",
    "\n",
    "def extract(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_text in tagged_texts:\n",
    "        tree = chunk_parser.parse(tagged_text)\n",
    "        tree = extract_chunks(tree)\n",
    "        tree = trees_to_tuples(tree)\n",
    "        chunks.append(tree)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def extract_chunks(tree):\n",
    "    chunks = []\n",
    "    \n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if tree.label() != 'S':  # Exclude sentence-level chunks if any\n",
    "            chunks.append(tree)\n",
    "        for subtree in tree:\n",
    "            chunks.extend(extract_chunks(subtree))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def trees_to_tuples(tree_list):\n",
    "    tuple_list = [tuple(leaf[0] for leaf in tree.leaves()) for tree in tree_list]\n",
    "    return tuple_list\n",
    "\n",
    "# Apply chunking and extraction to the 'tagged_sentences' column\n",
    "data['chunked_new_pattern'] = data['lemmatized'].apply(extract)\n",
    "\n",
    "# Print the resulting DataFrame with chunked data\n",
    "data['chunked_new_pattern']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasile\n",
    "\n",
    "patent_task = data[[\"title_and_abstract\", \"chunked_verb_noun\", \"chunked_new_pattern\"]]\n",
    "patent_task.to_excel('export_result/chunk1.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
