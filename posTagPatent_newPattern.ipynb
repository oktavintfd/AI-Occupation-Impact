{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \n",
       "0  The present disclosure relates to a fire detec...  \n",
       "1  The present disclosure relates to a fire detec...  \n",
       "2  Responsive to a CPU load of a specific access ...  \n",
       "3  Responsive to a CPU load of a specific access ...  \n",
       "4  Methods, apparatuses and computer readable med...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_csv('data/patent.csv')\n",
    "data = data[[\"title\", \"abstract_localized.text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to repair json data, wrap with [all data] and add coma at the end of each row, like try.json\n",
    "# import json\n",
    "\n",
    "# with open('data/try.json', 'r') as json_file:\n",
    "#     data_json = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "      <th>title_and_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>The present disclosure relates to a fire dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Responsive to a CPU load of a specific access...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \\\n",
       "0  The present disclosure relates to a fire detec...   \n",
       "1  The present disclosure relates to a fire detec...   \n",
       "2  Responsive to a CPU load of a specific access ...   \n",
       "3  Responsive to a CPU load of a specific access ...   \n",
       "4  Methods, apparatuses and computer readable med...   \n",
       "\n",
       "                                  title_and_abstract  \n",
       "0  Fire detection system based on artificial inte...  \n",
       "1   The present disclosure relates to a fire dete...  \n",
       "2  Embedding artificial intelligence for balancin...  \n",
       "3   Responsive to a CPU load of a specific access...  \n",
       "4  Optimization techniques for artificial intelli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_and_abstract'] = data['title'].fillna('') + ' ' + data['abstract_localized.text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                      234\n",
      "abstract_localized.text      0\n",
      "title_and_abstract           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing value\n",
    "\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove missing value\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Fire detection system based on artificial inte...\n",
       "1     The present disclosure relates to a fire dete...\n",
       "2    Embedding artificial intelligence for balancin...\n",
       "3     Responsive to a CPU load of a specific access...\n",
       "4    Optimization techniques for artificial intelli...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "data['cleaned'] = data['title_and_abstract'].apply(lambda x:remove_punctuation(str(x)))\n",
    "data['cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0      [Fire, detection, system, based, on, artificia...\n",
       "1      [The, present, disclosure, relates, to, a, fir...\n",
       "2      [Embedding, artificial, intelligence, for, bal...\n",
       "3      [Responsive, to, a, CPU, load, of, a, specific...\n",
       "4      [Optimization, techniques, for, artificial, in...\n",
       "                             ...                        \n",
       "495    [In, an, intelligent, system, for, providing, ...\n",
       "496    [Method, and, server, for, optimizing, hyperpa...\n",
       "497    [A, method, and, server, for, optimizing, hype...\n",
       "498    [Fallback, artificial, intelligence, system, f...\n",
       "499    [There, are, provided, systems, and, methods, ...\n",
       "Name: tokenized, Length: 500, dtype: object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "data['tokenized'] = data['title_and_abstract'].apply(nltk.word_tokenize)\n",
    "data['clean_tokenized'] = data['cleaned'].apply(nltk.word_tokenize)\n",
    "data['tokenized'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0      [Fire, detection, system, based, artificial, i...\n",
       "1      [The, present, disclosure, relates, fire, dete...\n",
       "2      [Embedding, artificial, intelligence, balancin...\n",
       "3      [Responsive, CPU, load, specific, access, poin...\n",
       "4      [Optimization, techniques, artificial, intelli...\n",
       "                             ...                        \n",
       "495    [In, intelligent, system, providing, recording...\n",
       "496    [Method, server, optimizing, hyperparameter, t...\n",
       "497    [A, method, server, optimizing, hyperparameter...\n",
       "498    [Fallback, artificial, intelligence, system, r...\n",
       "499    [There, provided, systems, methods, fallback, ...\n",
       "Name: clean_tokenized, Length: 500, dtype: object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "data['clean_tokenized'] = data['clean_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "data['clean_tokenized'].head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(Fire, NNP), (detection, NN), (system, NN), (...\n",
       "1    [(The, DT), (present, JJ), (disclosure, NN), (...\n",
       "2    [(Embedding, VBG), (artificial, JJ), (intellig...\n",
       "3    [(Responsive, JJ), (to, TO), (a, DT), (CPU, NN...\n",
       "4    [(Optimization, NN), (techniques, NNS), (for, ...\n",
       "Name: tagged, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tagged'] = data['tokenized'].apply(nltk.pos_tag)\n",
    "data['clean_tagged'] = data['clean_tokenized'].apply(nltk.pos_tag)\n",
    "data['tagged'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_tuples(tuples):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_tuples = []\n",
    "#     for tup in tuples:\n",
    "#         lemmatized_words = [lemmatizer.lemmatize(word) for word in tup]\n",
    "#         lemmatized_tuples.append(tuple(lemmatized_words))\n",
    "#     return lemmatized_tuples\n",
    "\n",
    "# data['lemmatized'] = data['tagged'].apply(lemmatize_tuples)\n",
    "# data['clean_lemmatized'] = data['clean_tagged'].apply(lemmatize_tuples)\n",
    "# data['lemmatized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [(Fire, NNP), (detection, NN), (system, NN), (...\n",
      "1      [(The, DT), (present, JJ), (disclosure, NN), (...\n",
      "2      [(Embedding, VBG), (artificial, JJ), (intellig...\n",
      "3      [(Responsive, JJ), (to, TO), (a, DT), (CPU, NN...\n",
      "4      [(Optimization, NN), (technique, NNS), (for, I...\n",
      "                             ...                        \n",
      "495    [(In, IN), (an, DT), (intelligent, NN), (syste...\n",
      "496    [(Method, NNP), (and, CC), (server, NN), (for,...\n",
      "497    [(A, DT), (method, NN), (and, CC), (server, NN...\n",
      "498    [(Fallback, NNP), (artificial, JJ), (intellige...\n",
      "499    [(There, EX), (be, VBP), (provide, VBN), (syst...\n",
      "Name: lemmatized, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # Default to noun if POS tag not recognized\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# data['lemmatized'] = data['tagged'].apply(\n",
    "#     lambda tagged_sentences: [(lemmatizer.lemmatize(word, get_wordnet_pos(tag)), tag) for word, tag in tagged_sentences])\n",
    "# print(data['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "\n",
    "# # Load spaCy language model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Function to lemmatize a list of tagged tokens\n",
    "# def lemmatize_tagged_sentences(tagged_sentences):\n",
    "#     lemmatized_sentences = []\n",
    "#     for token in tagged_sentences:\n",
    "#         word, pos = token\n",
    "#         lemmatized_word = nlp(word)[0].lemma_\n",
    "#         lemmatized_sentences.append((lemmatized_word, pos))\n",
    "#     return lemmatized_sentences\n",
    "\n",
    "# # Lemmatize the tagged sentences in each row of the DataFrame\n",
    "# data['lemmatized'] = data['tagged'].apply(lemmatize_tagged_sentences)\n",
    "\n",
    "# print(data['lemmatized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_word(word, tag):\n",
    "#     if tag.startswith('V'):\n",
    "#         return lemmatizer.lemmatize(word, 'v')  # Verb\n",
    "#     elif tag.startswith('N'):\n",
    "#         return lemmatizer.lemmatize(word, 'n')  # Noun\n",
    "#     else:\n",
    "#         return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos chunk lama\n",
    "\n",
    "# def filter_verb_noun_pairs(tagged_sentence):\n",
    "#     verb_noun_pairs = []\n",
    "#     for i in range(len(tagged_sentence) - 1):\n",
    "#         word, tag = tagged_sentence[i]\n",
    "#         next_word, next_tag = tagged_sentence[i + 1]\n",
    "#         if tag.startswith('VB') and next_tag.startswith('NN'):\n",
    "#             verb = lemmatize_word(word, tag)\n",
    "#             noun = lemmatize_word(next_word, next_tag)\n",
    "#             verb_noun_pairs.append((verb, noun))\n",
    "#     return verb_noun_pairs\n",
    "\n",
    "# data['chunked_verb_noun'] = data['clean_lemmatized'].apply(filter_verb_noun_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos chunk baru\n",
    "\n",
    "pattern = r'''Chunk: {(<ADJ|VB\\w*>+<.|RP|IN|CC|PRP\\w*>*<DET>*<NN\\w*|RB\\w*>+)+}'''\n",
    "chunk_parser = RegexpParser(pattern)\n",
    "\n",
    "def extract(tagged_text):\n",
    "    tree = chunk_parser.parse(tagged_text)\n",
    "    chunks = extract_chunks(tree)\n",
    "    return chunks\n",
    "\n",
    "def extract_chunks(tree):\n",
    "    chunks = []\n",
    "    \n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if tree.label() != 'S':  # Exclude sentence-level chunks if any\n",
    "            chunks.append(tree)\n",
    "        for subtree in tree:\n",
    "            chunks.extend(extract_chunks(subtree))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def trees_to_tuples(tree_list):\n",
    "    tuple_list = [tuple(leaf[0] for leaf in tree.leaves()) for tree in tree_list]\n",
    "    return tuple_list\n",
    "\n",
    "data['chunked'] = data['lemmatized'].apply(extract)\n",
    "data['chunked'] = data['chunked'].apply(trees_to_tuples)\n",
    "\n",
    "data['clean_chunked'] = data['clean_lemmatized'].apply(extract)\n",
    "data['clean_chunked'] = data['clean_chunked'].apply(trees_to_tuples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Result Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lowering text\n",
    "\n",
    "# data['msg_lower']= data['clean_msg'].apply(lambda x: x.lower())\n",
    "# data['msg_lower'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_tuples(tuples):\n",
    "    lowercased_tuples = []\n",
    "    for tup in tuples:\n",
    "        lowercased_words = tuple(word.lower() for word in tup)\n",
    "        lowercased_tuples.append(lowercased_words)\n",
    "    return lowercased_tuples\n",
    "\n",
    "data['chunked'] = data['chunked'].apply(lowercase_tuples)\n",
    "data['clean_chunked'] = data['clean_chunked'].apply(lowercase_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove stopwords\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     output= [i for i in text if i not in stopwords]\n",
    "#     return output\n",
    "\n",
    "# data['no_stopwords']= data['msg_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "# data['no_stopwords'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_word(word, tag):\n",
    "#     if tag.startswith('V'):\n",
    "#         return lemmatizer.lemmatize(word, 'v')  # Verb\n",
    "#     elif tag.startswith('N'):\n",
    "#         return lemmatizer.lemmatize(word, 'n')  # Noun\n",
    "#     else:\n",
    "#         return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['chunked_verb_noun'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Bismillah screepsweet\\NLP-AI-Impact\\posTagPatent_newPattern.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Bismillah%20screepsweet/NLP-AI-Impact/posTagPatent_newPattern.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# hasile\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Bismillah%20screepsweet/NLP-AI-Impact/posTagPatent_newPattern.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m patent_task \u001b[39m=\u001b[39m data[[\u001b[39m\"\u001b[39;49m\u001b[39mtitle_and_abstract\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mchunked_verb_noun\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mchunked\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mclean_chunked\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Bismillah%20screepsweet/NLP-AI-Impact/posTagPatent_newPattern.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m patent_task\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mexport_result/chunk.xlsx\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['chunked_verb_noun'] not in index\""
     ]
    }
   ],
   "source": [
    "# hasile\n",
    "\n",
    "patent_task = data[[\"title_and_abstract\", 'chunked_verb_noun', \"chunked\", 'clean_chunked']]\n",
    "patent_task.to_excel('export_result/chunk.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
