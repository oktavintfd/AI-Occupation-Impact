{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "string.punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A kind of artificial intelligence skin and its...</td>\n",
       "      <td>The invention discloses a kind of artificial i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A kind of image-recognizing method based on na...</td>\n",
       "      <td>The invention discloses a kind of image-recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The pedestrian detection method merged based o...</td>\n",
       "      <td>The present invention relates to a kind of ped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A kind of Knowledge Base based on natural lang...</td>\n",
       "      <td>The invention discloses a kind of Knowledge Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text emotion analysis method and device</td>\n",
       "      <td>The invention discloses a kind of text emotion...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A kind of artificial intelligence skin and its...   \n",
       "1  A kind of image-recognizing method based on na...   \n",
       "2  The pedestrian detection method merged based o...   \n",
       "3  A kind of Knowledge Base based on natural lang...   \n",
       "4            Text emotion analysis method and device   \n",
       "\n",
       "                                            abstract  \n",
       "0  The invention discloses a kind of artificial i...  \n",
       "1  The invention discloses a kind of image-recogn...  \n",
       "2  The present invention relates to a kind of ped...  \n",
       "3  The invention discloses a kind of Knowledge Ba...  \n",
       "4  The invention discloses a kind of text emotion...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data (data yang digunakan adalah data sample 500 baris agar pemrosesannya tidak lama)\n",
    "\n",
    "data = pd.read_excel('data/new_paten.xlsx')\n",
    "data = data[[\"title\", \"abstract\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_and_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A kind of artificial intelligence skin and its...</td>\n",
       "      <td>The invention discloses a kind of artificial i...</td>\n",
       "      <td>A kind of artificial intelligence skin and its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A kind of image-recognizing method based on na...</td>\n",
       "      <td>The invention discloses a kind of image-recogn...</td>\n",
       "      <td>A kind of image-recognizing method based on na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The pedestrian detection method merged based o...</td>\n",
       "      <td>The present invention relates to a kind of ped...</td>\n",
       "      <td>The pedestrian detection method merged based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A kind of Knowledge Base based on natural lang...</td>\n",
       "      <td>The invention discloses a kind of Knowledge Ba...</td>\n",
       "      <td>A kind of Knowledge Base based on natural lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text emotion analysis method and device.</td>\n",
       "      <td>The invention discloses a kind of text emotion...</td>\n",
       "      <td>Text emotion analysis method and device. The i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A kind of artificial intelligence skin and its...   \n",
       "1  A kind of image-recognizing method based on na...   \n",
       "2  The pedestrian detection method merged based o...   \n",
       "3  A kind of Knowledge Base based on natural lang...   \n",
       "4          Text emotion analysis method and device.    \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The invention discloses a kind of artificial i...   \n",
       "1  The invention discloses a kind of image-recogn...   \n",
       "2  The present invention relates to a kind of ped...   \n",
       "3  The invention discloses a kind of Knowledge Ba...   \n",
       "4  The invention discloses a kind of text emotion...   \n",
       "\n",
       "                                  title_and_abstract  \n",
       "0  A kind of artificial intelligence skin and its...  \n",
       "1  A kind of image-recognizing method based on na...  \n",
       "2  The pedestrian detection method merged based o...  \n",
       "3  A kind of Knowledge Base based on natural lang...  \n",
       "4  Text emotion analysis method and device. The i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add . in the end of title\n",
    "def add_period(text):\n",
    "    if pd.notna(text):\n",
    "        return text + \". \"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the 'title' column\n",
    "data['title'] = data['title'].apply(add_period)\n",
    "\n",
    "# merge title and abstract column\n",
    "data['title_and_abstract'] = data['title'].fillna('') + data['abstract']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                 0\n",
      "abstract              0\n",
      "title_and_abstract    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing value\n",
    "\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove missing value (tidak perlu karena kolom title and abstract tidak memiliki missing value)\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a kind of artificial intelligence skin and its...\n",
       "1    a kind of image-recognizing method based on na...\n",
       "2    the pedestrian detection method merged based o...\n",
       "3    a kind of knowledge base based on natural lang...\n",
       "4    text emotion analysis method and device. the i...\n",
       "Name: title_and_abstract, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowering text\n",
    "data['title_and_abstract']= data['title_and_abstract'].apply(lambda x: x.lower())\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['title_and_abstract'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'kind', 'of', 'artificial', 'intelligence', 'skin', 'and', 'its', 'method', 'for', 'detection', 'humiture', 'and', 'pressure'], ['the', 'invention', 'discloses', 'a', 'kind', 'of', 'artificial', 'intelligence', 'skin'], ['it', 'includes', 'signal', 'acquisition', 'part', 'and', 'circuit', 'part'], ['signal', 'acquisition', 'part', 'point', 'includes', 'humidity', 'sensitive', 'material', 'layer'], ['first', 'electrode', 'layer'], ['insulating', 'layer'], ['the', 'second', 'electrode', 'lay'], ['thermistor', 'material', 'layer'], ['the', '3rd', 'electrode', 'layer'], ['piezoelectric', 'material', 'layer', 'and', 'the', '4th', 'electrode', 'layer', 'set', 'gradually', 'from', 'top', 'to', 'bottom'], ['and', 'circuit', 'part', 'includes', 'front-end', 'circuit', 'and', 'back-end', 'circuit.the', 'invention', 'also', 'discloses', 'the', 'method', 'using', 'above-mentioned', 'artificial', 'intelligence', 'skin', 'detection', 'humiture', 'and', 'pressure.the', 'configuration', 'of', 'the', 'present', 'invention', 'is', 'simple'], ['be', 'easy', 'to', 'miniaturization', 'and', 'it', 'is', 'integrated'], ['test', 'method', 'is', 'succinct'], ['rigorous.the', 'present', 'invention', 'is', 'suitable', 'for', 'manufacturing', 'artificial', 'intelligence', 'skin']]\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize each sentence, tokenize each word from sentence, then remove stopwords\n",
    "def tokenize_and_remove_punctuation(text):\n",
    "    # Tokenize each sentence\n",
    "    sentences = re.split(r'(?<=[.!?,;])\\s+', text)\n",
    "    \n",
    "    # Remove punctuation and tokenize each word in each sentence\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each sentence and remove punctuation\n",
    "data['tokenized'] = data['title_and_abstract'].apply(tokenize_and_remove_punctuation)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['tokenized'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('method', 'NN'), ('for', 'IN'), ('detection', 'NN'), ('humiture', 'NN'), ('and', 'CC'), ('pressure', 'NN')], [('the', 'DT'), ('invention', 'NN'), ('discloses', 'VBZ'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN')], [('it', 'PRP'), ('includes', 'VBZ'), ('signal', 'JJ'), ('acquisition', 'NN'), ('part', 'NN'), ('and', 'CC'), ('circuit', 'NN'), ('part', 'NN')], [('signal', 'JJ'), ('acquisition', 'NN'), ('part', 'NN'), ('point', 'NN'), ('includes', 'VBZ'), ('humidity', 'NN'), ('sensitive', 'JJ'), ('material', 'NN'), ('layer', 'NN')], [('first', 'RB'), ('electrode', 'VBZ'), ('layer', 'NN')], [('insulating', 'VBG'), ('layer', 'NN')], [('the', 'DT'), ('second', 'JJ'), ('electrode', 'NN'), ('lay', 'VBD')], [('thermistor', 'NN'), ('material', 'NN'), ('layer', 'NN')], [('the', 'DT'), ('3rd', 'CD'), ('electrode', 'NN'), ('layer', 'NN')], [('piezoelectric', 'JJ'), ('material', 'NN'), ('layer', 'NN'), ('and', 'CC'), ('the', 'DT'), ('4th', 'CD'), ('electrode', 'NN'), ('layer', 'NN'), ('set', 'VBN'), ('gradually', 'RB'), ('from', 'IN'), ('top', 'JJ'), ('to', 'TO'), ('bottom', 'VB')], [('and', 'CC'), ('circuit', 'VB'), ('part', 'NN'), ('includes', 'VBZ'), ('front-end', 'JJ'), ('circuit', 'NN'), ('and', 'CC'), ('back-end', 'JJ'), ('circuit.the', 'NN'), ('invention', 'NN'), ('also', 'RB'), ('discloses', 'VBZ'), ('the', 'DT'), ('method', 'NN'), ('using', 'VBG'), ('above-mentioned', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'JJ'), ('detection', 'NN'), ('humiture', 'NN'), ('and', 'CC'), ('pressure.the', 'JJ'), ('configuration', 'NN'), ('of', 'IN'), ('the', 'DT'), ('present', 'JJ'), ('invention', 'NN'), ('is', 'VBZ'), ('simple', 'JJ')], [('be', 'VB'), ('easy', 'JJ'), ('to', 'TO'), ('miniaturization', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('integrated', 'VBN')], [('test', 'NN'), ('method', 'NN'), ('is', 'VBZ'), ('succinct', 'JJ')], [('rigorous.the', 'NN'), ('present', 'JJ'), ('invention', 'NN'), ('is', 'VBZ'), ('suitable', 'JJ'), ('for', 'IN'), ('manufacturing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to perform POS tagging on each token\n",
    "def pos_tag_tokens(tokenized_sentences):\n",
    "    pos_tagged_sentences = []\n",
    "    for sentence_tokens in tokenized_sentences:\n",
    "        pos_tags = pos_tag(sentence_tokens)\n",
    "        pos_tagged_sentences.append(pos_tags)\n",
    "    return pos_tagged_sentences\n",
    "\n",
    "# Perform POS tagging on tokenized sentences\n",
    "data['pos_tagged'] = data['tokenized'].apply(pos_tag_tokens)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['pos_tagged'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN'), ('and', 'CC'), ('it', 'PRP$'), ('method', 'NN'), ('for', 'IN'), ('detection', 'NN'), ('humiture', 'NN'), ('and', 'CC'), ('pressure', 'NN')], [('the', 'DT'), ('invention', 'NN'), ('disclose', 'VBZ'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN')], [('it', 'PRP'), ('include', 'VBZ'), ('signal', 'JJ'), ('acquisition', 'NN'), ('part', 'NN'), ('and', 'CC'), ('circuit', 'NN'), ('part', 'NN')], [('signal', 'JJ'), ('acquisition', 'NN'), ('part', 'NN'), ('point', 'NN'), ('include', 'VBZ'), ('humidity', 'NN'), ('sensitive', 'JJ'), ('material', 'NN'), ('layer', 'NN')], [('first', 'RB'), ('electrode', 'VBZ'), ('layer', 'NN')], [('insulate', 'VBG'), ('layer', 'NN')], [('the', 'DT'), ('second', 'JJ'), ('electrode', 'NN'), ('lay', 'VBD')], [('thermistor', 'NN'), ('material', 'NN'), ('layer', 'NN')], [('the', 'DT'), ('3rd', 'CD'), ('electrode', 'NN'), ('layer', 'NN')], [('piezoelectric', 'JJ'), ('material', 'NN'), ('layer', 'NN'), ('and', 'CC'), ('the', 'DT'), ('4th', 'CD'), ('electrode', 'NN'), ('layer', 'NN'), ('set', 'VBN'), ('gradually', 'RB'), ('from', 'IN'), ('top', 'JJ'), ('to', 'TO'), ('bottom', 'VB')], [('and', 'CC'), ('circuit', 'VB'), ('part', 'NN'), ('include', 'VBZ'), ('front-end', 'JJ'), ('circuit', 'NN'), ('and', 'CC'), ('back-end', 'JJ'), ('circuit.the', 'NN'), ('invention', 'NN'), ('also', 'RB'), ('disclose', 'VBZ'), ('the', 'DT'), ('method', 'NN'), ('use', 'VBG'), ('above-mentioned', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'JJ'), ('detection', 'NN'), ('humiture', 'NN'), ('and', 'CC'), ('pressure.the', 'JJ'), ('configuration', 'NN'), ('of', 'IN'), ('the', 'DT'), ('present', 'JJ'), ('invention', 'NN'), ('be', 'VBZ'), ('simple', 'JJ')], [('be', 'VB'), ('easy', 'JJ'), ('to', 'TO'), ('miniaturization', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('be', 'VBZ'), ('integrate', 'VBN')], [('test', 'NN'), ('method', 'NN'), ('be', 'VBZ'), ('succinct', 'JJ')], [('rigorous.the', 'NN'), ('present', 'JJ'), ('invention', 'NN'), ('be', 'VBZ'), ('suitable', 'JJ'), ('for', 'IN'), ('manufacture', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('skin', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(pos_tagged_sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence_tags in pos_tagged_sentences:\n",
    "        lemmatized_tokens = [(lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos_tag)), pos_tag) for token, pos_tag in sentence_tags]\n",
    "        lemmatized_sentences.append(lemmatized_tokens)\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Function to map POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN  # Default to noun if POS tag not recognized\n",
    "\n",
    "# Lemmatize the tokens in 'pos_tagged_sentences' column\n",
    "data['lemmatized'] = data['pos_tagged'].apply(lemmatize_tokens)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['lemmatized'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [(include, humidity), (electrode, layer), (ins...\n",
       "1         [(distribute, weight), (establish, information...\n",
       "2                    [(train, stage), (reside, background)]\n",
       "3         [(find, father), (define, sub-topicses), (perf...\n",
       "4         [(enhance, sentiment), (save, cost；sentiment),...\n",
       "                                ...                        \n",
       "138780    [(improve, performance), (recognise, traffic),...\n",
       "138781    [(acquire, surface), (derive, correction), (ca...\n",
       "138782                                                   []\n",
       "138783    [(predict, method), (predict, method), (lst, f...\n",
       "138784    [(microwave, source), (microwave, radiation), ...\n",
       "Name: chunked_verb_noun, Length: 138785, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos chunk verb noun pairs\n",
    "\n",
    "def extract_verb_noun_pairs(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_sentence in tagged_texts:\n",
    "        result = filter_verb_noun_pairs(tagged_sentence)\n",
    "        chunks.append(result)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def filter_verb_noun_pairs(tagged_sentence):\n",
    "    verb_noun_pairs = []\n",
    "    for i in range(len(tagged_sentence) - 1):\n",
    "        word, tag = tagged_sentence[i]\n",
    "        next_word, next_tag = tagged_sentence[i + 1]\n",
    "        if tag.startswith('VB') and next_tag.startswith('NN'):\n",
    "            verb = word\n",
    "            noun = next_word\n",
    "            verb_noun_pairs.append((verb, noun))\n",
    "    return verb_noun_pairs\n",
    "\n",
    "# Apply chunking and extraction to the 'tagged_sentences' column\n",
    "data['chunked_verb_noun'] = data['lemmatized'].apply(extract_verb_noun_pairs)\n",
    "\n",
    "# Print the resulting DataFrame with chunked data\n",
    "data['chunked_verb_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [(disclose, a, kind), (include, humidity), (el...\n",
       "1         [(disclose, a, kind), (be, distribute, weight)...\n",
       "2         [(train, stage), (scene, simultaneously, mark,...\n",
       "3         [(disclose, a, kind), (be, put, into, default,...\n",
       "4         [(disclose, a, kind), (dictionary；based, on, t...\n",
       "                                ...                        \n",
       "138780    [(improve, performance), (recognise, traffic, ...\n",
       "138781    [(provide, a, method), (be, perform, by, a, co...\n",
       "138782    [(comprise, a, head, mask, structure), (protec...\n",
       "138783    [(predict, method), (base, on, machine, learni...\n",
       "138784    [(microwave, source), (microwave, radiation, s...\n",
       "Name: chunked_new_pattern, Length: 138785, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS chunk new pattern (Pak Indra)\n",
    "\n",
    "# define pattern\n",
    "pattern = r'''Chunk: {(<ADJ|VB\\w*>+<\\.|RP|IN|CC|PRP\\w*>*<DT>*<NN\\w*|RB\\w*>+)+}'''\n",
    "chunk_parser = RegexpParser(pattern)\n",
    "\n",
    "def extract(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_text in tagged_texts:\n",
    "        tree = chunk_parser.parse(tagged_text)\n",
    "        tree = extract_chunks(tree)\n",
    "        tree = trees_to_tuples(tree)\n",
    "        chunks.append(tree)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def extract_chunks(tree):\n",
    "    chunks = []\n",
    "    \n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if tree.label() != 'S':  # Exclude sentence-level chunks if any\n",
    "            chunks.append(tree)\n",
    "        for subtree in tree:\n",
    "            chunks.extend(extract_chunks(subtree))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def trees_to_tuples(tree_list):\n",
    "    tuple_list = [tuple(leaf[0] for leaf in tree.leaves()) for tree in tree_list]\n",
    "    return tuple_list\n",
    "\n",
    "# Apply chunking and extraction to the 'tagged_sentences' column\n",
    "data['chunked_new_pattern'] = data['lemmatized'].apply(extract)\n",
    "\n",
    "# Print the resulting DataFrame with chunked data\n",
    "data['chunked_new_pattern']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasile\n",
    "\n",
    "patent_task = data[[\"title_and_abstract\", \"chunked_verb_noun\", \"chunked_new_pattern\"]]\n",
    "patent_task.to_excel('export_result/paten_task_new.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
