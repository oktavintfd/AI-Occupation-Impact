{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "string.punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \n",
       "0  The present disclosure relates to a fire detec...  \n",
       "1  The present disclosure relates to a fire detec...  \n",
       "2  Responsive to a CPU load of a specific access ...  \n",
       "3  Responsive to a CPU load of a specific access ...  \n",
       "4  Methods, apparatuses and computer readable med...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_csv('data/patent.csv')\n",
    "data = data[[\"title\", \"abstract_localized.text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add . in the end of title\n",
    "\n",
    "def add_period(text):\n",
    "    if pd.notna(text):\n",
    "        return text + \". \"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the 'title' column\n",
    "data['title'] = data['title'].apply(add_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to repair json data, wrap with [all data] and add coma at the end of each row, like try.json\n",
    "# import json\n",
    "\n",
    "# with open('data/try.json', 'r') as json_file:\n",
    "#     data_json = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_localized.text</th>\n",
       "      <th>title_and_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>Fire detection system based on artificial inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "      <td>The present disclosure relates to a fire detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Embedding artificial intelligence for balancin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "      <td>Responsive to a CPU load of a specific access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "      <td>Methods, apparatuses and computer readable med...</td>\n",
       "      <td>Optimization techniques for artificial intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fire detection system based on artificial inte...   \n",
       "1                                                NaN   \n",
       "2  Embedding artificial intelligence for balancin...   \n",
       "3                                                NaN   \n",
       "4  Optimization techniques for artificial intelli...   \n",
       "\n",
       "                             abstract_localized.text  \\\n",
       "0  The present disclosure relates to a fire detec...   \n",
       "1  The present disclosure relates to a fire detec...   \n",
       "2  Responsive to a CPU load of a specific access ...   \n",
       "3  Responsive to a CPU load of a specific access ...   \n",
       "4  Methods, apparatuses and computer readable med...   \n",
       "\n",
       "                                  title_and_abstract  \n",
       "0  Fire detection system based on artificial inte...  \n",
       "1  The present disclosure relates to a fire detec...  \n",
       "2  Embedding artificial intelligence for balancin...  \n",
       "3  Responsive to a CPU load of a specific access ...  \n",
       "4  Optimization techniques for artificial intelli...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_and_abstract'] = data['title'].fillna('') + data['abstract_localized.text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                      234\n",
      "abstract_localized.text      0\n",
      "title_and_abstract           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing value\n",
    "\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove missing value\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove punctuation\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "#     return punctuationfree\n",
    "\n",
    "# data['cleaned'] = data['title_and_abstract'].apply(lambda x:remove_punctuation(str(x)))\n",
    "# data['cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fire detection system based on artificial inte...\n",
       "1    the present disclosure relates to a fire detec...\n",
       "2    embedding artificial intelligence for balancin...\n",
       "3    responsive to a cpu load of a specific access ...\n",
       "4    optimization techniques for artificial intelli...\n",
       "Name: title_and_abstract, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowering text\n",
    "\n",
    "data['title_and_abstract']= data['title_and_abstract'].apply(lambda x: x.lower())\n",
    "data['title_and_abstract'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenization\n",
    "\n",
    "# data['tokenized'] = data['title_and_abstract'].apply(nltk.word_tokenize)\n",
    "# data['clean_tokenized'] = data['cleaned'].apply(nltk.word_tokenize)\n",
    "# data['tokenized'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize each sentence\n",
    "# data['sentences'] = data['title_and_abstract'].apply(sent_tokenize)\n",
    "\n",
    "# # Tokenize each word within each sentence\n",
    "# data['tokenized'] = data['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(data[['sentences', 'tokenized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fire', 'detection', 'system', 'based', 'on', 'artificial', 'intelligence', 'and', 'fire', 'detection', 'method', 'based', 'on', 'artificial', 'intelligence'], ['the', 'present', 'disclosure', 'relates', 'to', 'a', 'fire', 'detection', 'system', 'based', 'on', 'ai', 'and', 'a', 'fire', 'detection', 'method', 'based', 'on', 'ai', 'which', 'detects', 'fire', 'by', 'using', 'deep', 'learning'], ['the', 'fire', 'detection', 'system', 'includes', 'a', 'camera', 'part', 'generating', 'video', 'images', 'an', 'object', 'extraction', 'part', 'extracting', 'a', 'motion', 'object', 'from', 'the', 'video', 'images', 'a', 'video', 'image', 'conversion', 'part', 'generating', 'a', 'first', 'background', 'removal', 'image', 'an', 'image', 'division', 'part', 'dividing', 'the', 'first', 'background', 'removal', 'image', 'with', 'a', 'plurality', 'of', 'division', 'lines', 'and', 'an', 'analyzing', 'part', 'generating', 'an', 'abnormal', 'signal', 'and', 'a', 'normal', 'signal']]\n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation and tokenize each sentence\n",
    "def tokenize_and_remove_punctuation(text):\n",
    "    # Tokenize each sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and tokenize each word in each sentence\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize each sentence and remove punctuation\n",
    "data['tokenized'] = data['title_and_abstract'].apply(tokenize_and_remove_punctuation)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['tokenized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove stopwords\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     output= [i for i in text if i not in stopwords]\n",
    "#     return output\n",
    "\n",
    "# data['clean_tokenized'] = data['clean_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "# data['clean_tokenized'].head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tagged'] = data['tokenized'].apply(nltk.pos_tag)\n",
    "# data['clean_tagged'] = data['clean_tokenized'].apply(nltk.pos_tag)\n",
    "# data['tagged'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')], [('the', 'DT'), ('present', 'JJ'), ('disclosure', 'NN'), ('relates', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('and', 'CC'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('which', 'WDT'), ('detects', 'VBZ'), ('fire', 'NN'), ('by', 'IN'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'NN')], [('the', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('includes', 'VBZ'), ('a', 'DT'), ('camera', 'NN'), ('part', 'NN'), ('generating', 'VBG'), ('video', 'NN'), ('images', 'NNS'), ('an', 'DT'), ('object', 'JJ'), ('extraction', 'NN'), ('part', 'NN'), ('extracting', 'VBG'), ('a', 'DT'), ('motion', 'NN'), ('object', 'NN'), ('from', 'IN'), ('the', 'DT'), ('video', 'NN'), ('images', 'VBZ'), ('a', 'DT'), ('video', 'NN'), ('image', 'NN'), ('conversion', 'NN'), ('part', 'NN'), ('generating', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('an', 'DT'), ('image', 'NN'), ('division', 'NN'), ('part', 'NN'), ('dividing', 'VBG'), ('the', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('with', 'IN'), ('a', 'DT'), ('plurality', 'NN'), ('of', 'IN'), ('division', 'NN'), ('lines', 'NNS'), ('and', 'CC'), ('an', 'DT'), ('analyzing', 'VBG'), ('part', 'NN'), ('generating', 'VBG'), ('an', 'DT'), ('abnormal', 'JJ'), ('signal', 'NN'), ('and', 'CC'), ('a', 'DT'), ('normal', 'JJ'), ('signal', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to perform POS tagging on each token\n",
    "def pos_tag_tokens(tokenized_sentences):\n",
    "    pos_tagged_sentences = []\n",
    "    for sentence_tokens in tokenized_sentences:\n",
    "        pos_tags = pos_tag(sentence_tokens)\n",
    "        pos_tagged_sentences.append(pos_tags)\n",
    "    return pos_tagged_sentences\n",
    "\n",
    "# Perform POS tagging on tokenized sentences\n",
    "data['pos_tagged'] = data['tokenized'].apply(pos_tag_tokens)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(data['pos_tagged'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')], [('the', 'DT'), ('present', 'JJ'), ('disclosure', 'NN'), ('relate', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('and', 'CC'), ('a', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('method', 'NN'), ('base', 'VBN'), ('on', 'IN'), ('ai', 'NN'), ('which', 'WDT'), ('detect', 'VBZ'), ('fire', 'NN'), ('by', 'IN'), ('use', 'VBG'), ('deep', 'JJ'), ('learning', 'NN')], [('the', 'DT'), ('fire', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('include', 'VBZ'), ('a', 'DT'), ('camera', 'NN'), ('part', 'NN'), ('generate', 'VBG'), ('video', 'NN'), ('image', 'NNS'), ('an', 'DT'), ('object', 'JJ'), ('extraction', 'NN'), ('part', 'NN'), ('extract', 'VBG'), ('a', 'DT'), ('motion', 'NN'), ('object', 'NN'), ('from', 'IN'), ('the', 'DT'), ('video', 'NN'), ('image', 'VBZ'), ('a', 'DT'), ('video', 'NN'), ('image', 'NN'), ('conversion', 'NN'), ('part', 'NN'), ('generate', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('an', 'DT'), ('image', 'NN'), ('division', 'NN'), ('part', 'NN'), ('divide', 'VBG'), ('the', 'DT'), ('first', 'JJ'), ('background', 'NN'), ('removal', 'NN'), ('image', 'NN'), ('with', 'IN'), ('a', 'DT'), ('plurality', 'NN'), ('of', 'IN'), ('division', 'NN'), ('line', 'NNS'), ('and', 'CC'), ('an', 'DT'), ('analyze', 'VBG'), ('part', 'NN'), ('generate', 'VBG'), ('an', 'DT'), ('abnormal', 'JJ'), ('signal', 'NN'), ('and', 'CC'), ('a', 'DT'), ('normal', 'JJ'), ('signal', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Function to lemmatize tokens while preserving the POS tags\n",
    "def lemmatize_tokens(pos_tagged_sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence_tags in pos_tagged_sentences:\n",
    "        lemmatized_tokens = [(lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos_tag)), pos_tag) for token, pos_tag in sentence_tags]\n",
    "        lemmatized_sentences.append(lemmatized_tokens)\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Function to map POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN  # Default to noun if POS tag not recognized\n",
    "\n",
    "# Lemmatize the tokens in 'pos_tagged_sentences' column\n",
    "data['lemmatized'] = data['pos_tagged'].apply(lemmatize_tokens)\n",
    "print(data['lemmatized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_tuples(tuples):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_tuples = []\n",
    "#     for tup in tuples:\n",
    "#         lemmatized_words = [lemmatizer.lemmatize(word) for word in tup]\n",
    "#         lemmatized_tuples.append(tuple(lemmatized_words))\n",
    "#     return lemmatized_tuples\n",
    "\n",
    "# data['lemmatized'] = data['tagged'].apply(lemmatize_tuples)\n",
    "# data['clean_lemmatized'] = data['clean_tagged'].apply(lemmatize_tuples)\n",
    "# data['lemmatized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # Default to noun if POS tag not recognized\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# data['lemmatized'] = data['tagged'].apply(\n",
    "#     lambda tagged_sentences: [(lemmatizer.lemmatize(word, get_wordnet_pos(tag)), tag) for word, tag in tagged_sentences])\n",
    "# print(data['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "\n",
    "# # Load spaCy language model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Function to lemmatize a list of tagged tokens\n",
    "# def lemmatize_tagged_sentences(tagged_sentences):\n",
    "#     lemmatized_sentences = []\n",
    "#     for token in tagged_sentences:\n",
    "#         word, pos = token\n",
    "#         lemmatized_word = nlp(word)[0].lemma_\n",
    "#         lemmatized_sentences.append((lemmatized_word, pos))\n",
    "#     return lemmatized_sentences\n",
    "\n",
    "# # Lemmatize the tagged sentences in each row of the DataFrame\n",
    "# data['lemmatized'] = data['tagged'].apply(lemmatize_tagged_sentences)\n",
    "\n",
    "# print(data['lemmatized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_word(word, tag):\n",
    "#     if tag.startswith('V'):\n",
    "#         return lemmatizer.lemmatize(word, 'v')  # Verb\n",
    "#     elif tag.startswith('N'):\n",
    "#         return lemmatizer.lemmatize(word, 'n')  # Noun\n",
    "#     else:\n",
    "#         return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos chunk lama\n",
    "\n",
    "# def filter_verb_noun_pairs(tagged_sentence):\n",
    "#     verb_noun_pairs = []\n",
    "#     for i in range(len(tagged_sentence) - 1):\n",
    "#         word, tag = tagged_sentence[i]\n",
    "#         next_word, next_tag = tagged_sentence[i + 1]\n",
    "#         if tag.startswith('VB') and next_tag.startswith('NN'):\n",
    "#             verb = lemmatize_word(word, tag)\n",
    "#             noun = lemmatize_word(next_word, next_tag)\n",
    "#             verb_noun_pairs.append((verb, noun))\n",
    "#     return verb_noun_pairs\n",
    "\n",
    "# data['chunked_verb_noun'] = data['clean_lemmatized'].apply(filter_verb_noun_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(detect, fire), (generate, video), (analyze, ...\n",
       "1      [(detect, fire), (generate, video), (analyze, ...\n",
       "2      [(balance, access), (process, load), (enable, ...\n",
       "3                [(threshold, value), (determine, rssi)]\n",
       "4      [(receive, annotation), (determine, document),...\n",
       "                             ...                        \n",
       "495                [(learn, model), (further, comprise)]\n",
       "496    [(optimize, hyperparameter), (optimize, hyperp...\n",
       "497    [(optimize, hyperparameter), (ai, model), (eva...\n",
       "498    [(provide, system), (provide, ai), (compute, e...\n",
       "499    [(provide, system), (provide, ai), (compute, e...\n",
       "Name: chunked_verb_noun, Length: 500, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos chunk lama\n",
    "\n",
    "def extract_verb_noun_pairs(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_sentence in tagged_texts:\n",
    "        result = filter_verb_noun_pairs(tagged_sentence)\n",
    "        chunks.append(result)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def filter_verb_noun_pairs(tagged_sentence):\n",
    "    verb_noun_pairs = []\n",
    "    for i in range(len(tagged_sentence) - 1):\n",
    "        word, tag = tagged_sentence[i]\n",
    "        next_word, next_tag = tagged_sentence[i + 1]\n",
    "        if tag.startswith('VB') and next_tag.startswith('NN'):\n",
    "            verb = word\n",
    "            noun = next_word\n",
    "            verb_noun_pairs.append((verb, noun))\n",
    "    return verb_noun_pairs\n",
    "\n",
    "data['chunked_verb_noun'] = data['lemmatized'].apply(extract_verb_noun_pairs)\n",
    "data['chunked_verb_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos chunk baru\n",
    "\n",
    "# pattern = r'''Chunk: {(<ADJ|VB\\w*>+<.|RP|IN|CC|PRP\\w*>*<DET>*<NN\\w*|RB\\w*>+)+}'''\n",
    "# chunk_parser = RegexpParser(pattern)\n",
    "\n",
    "# def extract(tagged_text):\n",
    "#     tree = chunk_parser.parse(tagged_text)\n",
    "#     chunks = extract_chunks(tree)\n",
    "#     return chunks\n",
    "\n",
    "# def extract_chunks(tree):\n",
    "#     chunks = []\n",
    "    \n",
    "#     if isinstance(tree, nltk.Tree):\n",
    "#         if tree.label() != 'S':  # Exclude sentence-level chunks if any\n",
    "#             chunks.append(tree)\n",
    "#         for subtree in tree:\n",
    "#             chunks.extend(extract_chunks(subtree))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# def trees_to_tuples(tree_list):\n",
    "#     tuple_list = [tuple(leaf[0] for leaf in tree.leaves()) for tree in tree_list]\n",
    "#     return tuple_list\n",
    "\n",
    "# data['chunked'] = data['lemmatized'].apply(extract)\n",
    "# data['chunked'] = data['chunked'].apply(trees_to_tuples)\n",
    "\n",
    "# data['clean_chunked'] = data['clean_lemmatized'].apply(extract)\n",
    "# data['clean_chunked'] = data['clean_chunked'].apply(trees_to_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(base, on, ai), (base, on, ai), (detect, fire...\n",
       "1      [(base, on, ai), (base, on, ai), (detect, fire...\n",
       "2      [(balance, access, point, process, load), (ena...\n",
       "3      [(threshold, value), (determine, rssi, value),...\n",
       "4      [(comprise, select, from, a, pool), (be, annot...\n",
       "                             ...                        \n",
       "495    [(comprise, receive, from, a, client, device),...\n",
       "496    [(optimize, hyperparameter, tuples), (optimize...\n",
       "497    [(optimize, hyperparameter, tuples), (ai, mode...\n",
       "498    [(be, provide, system), (provide, ai, system),...\n",
       "499    [(be, provide, system), (provide, ai, system),...\n",
       "Name: chunked, Length: 500, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the chunking pattern\n",
    "pattern = r'''Chunk: {(<ADJ|VB\\w*>+<\\.|RP|IN|CC|PRP\\w*>*<DT>*<NN\\w*|RB\\w*>+)+}'''\n",
    "chunk_parser = RegexpParser(pattern)\n",
    "\n",
    "def extract(tagged_texts):\n",
    "    chunks = []\n",
    "    for tagged_text in tagged_texts:\n",
    "        tree = chunk_parser.parse(tagged_text)\n",
    "        tree = extract_chunks(tree)\n",
    "        tree = trees_to_tuples(tree)\n",
    "        chunks.append(tree)\n",
    "    chunks = [item for sublist in chunks for item in sublist]\n",
    "    return chunks\n",
    "\n",
    "def extract_chunks(tree):\n",
    "    chunks = []\n",
    "    \n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if tree.label() != 'S':  # Exclude sentence-level chunks if any\n",
    "            chunks.append(tree)\n",
    "        for subtree in tree:\n",
    "            chunks.extend(extract_chunks(subtree))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def trees_to_tuples(tree_list):\n",
    "    tuple_list = [tuple(leaf[0] for leaf in tree.leaves()) for tree in tree_list]\n",
    "    return tuple_list\n",
    "\n",
    "# Apply chunking and extraction to the 'tagged_sentences' column\n",
    "data['chunked'] = data['lemmatized'].apply(extract)\n",
    "# data['chunked'] = data['chunked'].apply(trees_to_tuples)\n",
    "\n",
    "# Print the resulting DataFrame with chunked data\n",
    "data['chunked']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Result Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lowering text\n",
    "\n",
    "# data['msg_lower']= data['clean_msg'].apply(lambda x: x.lower())\n",
    "# data['msg_lower'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowercase_tuples(tuples):\n",
    "#     lowercased_tuples = []\n",
    "#     for tup in tuples:\n",
    "#         lowercased_words = tuple(word.lower() for word in tup)\n",
    "#         lowercased_tuples.append(lowercased_words)\n",
    "#     return lowercased_tuples\n",
    "\n",
    "# data['chunked'] = data['chunked'].apply(lowercase_tuples)\n",
    "# data['clean_chunked'] = data['clean_chunked'].apply(lowercase_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowercase_lists(lists):\n",
    "#     chunks = []\n",
    "#     for list in lists:\n",
    "#         result = lowercase_tuples(list)\n",
    "#         chunks.append(result)\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# def lowercase_tuples(tuples):\n",
    "#     lowercased_tuples = []\n",
    "#     for tup in tuples:\n",
    "#         lowercased_words = tuple(word.lower() for word in tup)\n",
    "#         lowercased_tuples.append(lowercased_words)\n",
    "#     return lowercased_tuples\n",
    "\n",
    "# data['chunked'] = data['chunked'].apply(lowercase_tuples)\n",
    "# data['chunked_verb_noun'] = data['chunked_verb_noun'].apply(lowercase_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove stopwords\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     output= [i for i in text if i not in stopwords]\n",
    "#     return output\n",
    "\n",
    "# data['no_stopwords']= data['msg_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "# data['no_stopwords'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_word(word, tag):\n",
    "#     if tag.startswith('V'):\n",
    "#         return lemmatizer.lemmatize(word, 'v')  # Verb\n",
    "#     elif tag.startswith('N'):\n",
    "#         return lemmatizer.lemmatize(word, 'n')  # Noun\n",
    "#     else:\n",
    "#         return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasile\n",
    "\n",
    "patent_task = data[[\"title_and_abstract\", \"chunked_verb_noun\", \"chunked\"]]\n",
    "patent_task.to_excel('export_result/chunk1.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
